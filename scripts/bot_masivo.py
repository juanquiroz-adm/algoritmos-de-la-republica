import requests
import pdfplumber
import re
import os
import time
from collections import Counter

# 1. CONFIGURACI√ìN DEL CORPUS M√öLTIPLE
# Usamos 3 documentos pol√≠ticos/legales colombianos reales alojados en servidores estables (OEA)
# En tu proyecto, esta lista se llenar√° sola con la Ara√±a.
urls_objetivos = [
    "https://www.oas.org/juridico/spanish/mesicic2_col_ley_190_1995.pdf", # Doc 1: Estatuto Anticorrupci√≥n
    "https://www.oas.org/juridico/spanish/mesicic2_col_ley_489_1998.pdf", # Doc 2: Ley de Administraci√≥n
    "https://www.oas.org/juridico/spanish/mesicic2_col_constitucion.pdf"  # Doc 3: Constituci√≥n de Colombia
]

archivo_temporal = "documento_en_transito.pdf"
stopwords = ['de', 'la', 'el', 'en', 'y', 'a', 'los', 'que', 'por', 'las', 'con', 'un', 'para', 'una', 'su', 'se', 'del', 'al', 'es', 'como', 'o', 'sus', 'no', 'lo', 'o', 'las', 'como', 'm√°s']

# Aqu√≠ guardaremos el texto de TODOS los documentos combinados
corpus_maestro = ""

print("=== INICIANDO EXTRACCI√ìN MASIVA (BATCH PROCESSING) ===")
print(f"Total de documentos en la cola: {len(urls_objetivos)}\n")

# Simulamos ser un navegador real
headers = {"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0"}

try:
    # 2. EL BUCLE DE AUTOMATIZACI√ìN (El coraz√≥n del bot)
    for indice, url in enumerate(urls_objetivos, start=1):
        print(f"‚è≥ [Descargando {indice}/{len(urls_objetivos)}] Conectando a servidor...")
        
        # Descarga
        respuesta = requests.get(url, headers=headers, timeout=30)
        if respuesta.status_code == 200:
            with open(archivo_temporal, 'wb') as f:
                f.write(respuesta.content)
            print(f"   ‚úÖ Descarga OK. Extrayendo texto (esto puede tomar unos segundos)...")
            
            # Extracci√≥n
            with pdfplumber.open(archivo_temporal) as pdf:
                # Para la prueba, leeremos solo las primeras 5 p√°ginas de cada PDF 
                # (La Constituci√≥n tiene 140 p√°ginas, queremos que el script termine hoy)
                paginas_a_leer = pdf.pages[:5] 
                
                for pagina in paginas_a_leer:
                    texto = pagina.extract_text()
                    if texto:
                        corpus_maestro += texto + " "
            
            # Limpieza del disco
            if os.path.exists(archivo_temporal):
                os.remove(archivo_temporal)
            print(f"   ‚úÖ Texto extra√≠do y archivo temporal destruido.")
            
            # Pausa de 2 segundos para no saturar el servidor (Cortes√≠a de Scraping)
            time.sleep(2)
        else:
            print(f"   ‚ùå Error HTTP {respuesta.status_code} en Doc {indice}")

    # 3. PROCESAMIENTO NLP DEL CORPUS MAESTRO
    print("\nüß† [PROCESANDO BIG DATA] Limpiando el corpus unificado...")
    texto_limpio = corpus_maestro.lower()
    # Dejamos solo letras, quitamos n√∫meros y s√≠mbolos
    texto_limpio = re.sub(r'[^a-z√°√©√≠√≥√∫√±]+', ' ', texto_limpio)
    
    palabras = texto_limpio.split()
    # Filtramos palabras in√∫tiles y muy cortas
    palabras_utiles = [p for p in palabras if p not in stopwords and len(p) > 3]
    
    print(f"‚úÖ Limpieza completada. Total de palabras clave extra√≠das de los 3 documentos: {len(palabras_utiles)}")

    # 4. AN√ÅLISIS ESTAD√çSTICO
    print("\nüìä RESULTADOS: TOP 10 PALABRAS M√ÅS FRECUENTES EN TODO EL CORPUS:")
    contador = Counter(palabras_utiles)
    top_10 = contador.most_common(10)

    for palabra, frecuencia in top_10:
        print(f" -> {palabra.upper()}: {frecuencia} veces")

    print("\n‚úÖ EJECUCI√ìN DEL BUCLE 100% COMPLETADA. EL SISTEMA ES OPERATIVO.")

except Exception as e:
    print(f"\n‚ùå ERROR CR√çTICO DURANTE LA EJECUCI√ìN: {e}")